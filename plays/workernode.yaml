---
- name: Install and Configure Hadoop
  hosts: all
  become: yes
  vars:
    hadoop_home: "/usr/local/hadoop"
    java_home: "/usr/lib/jvm/java-8-openjdk-amd64"

  tasks:

    ## ðŸŸ¢ Ã‰tape 1 : Installation de Java et des dÃ©pendances
    - name: Update package lists
      apt:
        update_cache: yes

    - name: Install required packages
      apt:
        name:
          - openjdk-8-jdk
          - ssh
          - rsync
        state: present

    - name: Set JAVA_HOME in environment variables
      lineinfile:
        path: /etc/environment
        regexp: '^JAVA_HOME='
        line: 'JAVA_HOME={{ java_home }}'
        state: present

    - name: Apply environment changes
      shell: source /etc/environment
      args:
        executable: /bin/bash

    ## ðŸŸ¢ Ã‰tape 2 : TÃ©lÃ©chargement et Extraction de Hadoop
    - name: Download Hadoop
      get_url:
        url: "https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz"
        dest: "/tmp/hadoop.tar.gz"

    - name: Extract Hadoop
      unarchive:
        src: "/tmp/hadoop.tar.gz"
        dest: "/usr/local/"
        remote_src: yes

    - name: Rename Hadoop directory
      command: mv /usr/local/hadoop-3.3.6 /usr/local/hadoop
      args:
        removes: /usr/local/hadoop-3.3.6
        creates: "{{ hadoop_home }}"

    ## ðŸŸ¢ Ã‰tape 3 : Configuration de `JAVA_HOME` dans Hadoop
    - name: Set JAVA_HOME in Hadoop environment
      lineinfile:
        path: "{{ hadoop_home }}/etc/hadoop/hadoop-env.sh"
        regexp: '^export JAVA_HOME='
        line: 'export JAVA_HOME={{ java_home }}'
        state: present

    ## ðŸŸ¢ Ã‰tape 4 : Configuration des fichiers XML de Hadoop
    - name: Configure core-site.xml
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/core-site.xml"
        content: |
          <configuration>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://localhost:9000</value>
            </property>
          </configuration>

    - name: Configure hdfs-site.xml
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/hdfs-site.xml"
        content: |
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>1</value>
            </property>
            <property>
              <name>dfs.namenode.name.dir</name>
              <value>file:/usr/local/hadoop/data/namenode</value>
            </property>
            <property>
              <name>dfs.datanode.data.dir</name>
              <value>file:/usr/local/hadoop/data/datanode</value>
            </property>
          </configuration>

    - name: Configure mapred-site.xml
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/mapred-site.xml"
        content: |
          <configuration>
            <property>
              <name>mapreduce.framework.name</name>
              <value>yarn</value>
            </property>
          </configuration>

    - name: Configure yarn-site.xml
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/yarn-site.xml"
        content: |
          <configuration>
            <property>
              <name>yarn.nodemanager.aux-services</name>
              <value>mapreduce_shuffle</value>
            </property>
          </configuration>


