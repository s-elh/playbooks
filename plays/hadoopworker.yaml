---
- hosts: all
  become: yes
  vars:
    master_ip: "morpehus[instance][containers].findAll{it.containerTypeShortName == 'master'}.collect{it.externalIp}"
    proxy_url: "http://10.1.38.2:3128"

  tasks:
  
    # Step 2: Create Hadoop user
    - name: Add Hadoop user
      user:
        name: hadoop
        state: present
        groups: sudo
        shell: /bin/bash
        password: ACSCloud!2021!
        
    - name: Configure HTTP proxy for apt
      copy:
        content: |
          Acquire::http::Proxy "{{ proxy_url }}";
          Acquire::https::Proxy "{{ proxy_url }}";
        dest: /etc/apt/apt.conf.d/95proxies

    - name: Configure HTTP proxy for the environment
      lineinfile:
        path: /etc/environment
        line: '{{ item }}'
        create: yes
      with_items:
        - 'http_proxy={{ proxy_url }}'
        - 'https_proxy={{ proxy_url }}'
        - 'HTTP_PROXY={{ proxy_url }}'
        - 'HTTPS_PROXY={{ proxy_url }}'
    # Step 1: Install Java
    - name: Install OpenJDK 8
      apt:
        name: openjdk-8-jdk
        state: present
        update_cache: yes
    

    
    # Step 3: Install SSH
    - name: Install SSH
      apt:
        name: openssh-server
        state: present
        update_cache: yes

    # Step 4: Download and install Hadoop
    - name: Download Hadoop
      get_url:
        url: https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
        dest: /tmp/hadoop-3.3.6.tar.gz

    - name: Extract Hadoop
      unarchive:
        src: /tmp/hadoop-3.3.6.tar.gz
        dest: /usr/local
        remote_src: yes

    - name: Create Hadoop symlink
      file:
        src: /usr/local/hadoop-3.3.6
        dest: /usr/local/hadoop
        state: link

    # Step 5: Set environment variables
    - name: Configure Hadoop environment variables
      lineinfile:
        path: /home/hadoop/.bashrc
        line: "{{ item }}"
        state: present
      with_items:
        - 'export HADOOP_HOME=/usr/local/hadoop'
        - 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin'
        - 'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop'
        - 'export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")'
    
    - name: Source the environment variables
      command: source /home/hadoop/.bashrc

    # Step 6: Configure Hadoop's core-site.xml
    - name: Configure core-site.xml with Master IP
      copy:
        dest: /usr/local/hadoop/etc/hadoop/core-site.xml
        content: |
          <configuration>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://{{ master_ip }}:9000</value>
            </property>
          </configuration>
      
    
    # Step 7: Configure Hadoop's yarn-site.xml
    - name: Configure yarn-site.xml with Master IP
      copy:
        dest: /usr/local/hadoop/etc/hadoop/yarn-site.xml
        content: |
          <configuration>
            <property>
              <name>yarn.resourcemanager.hostname</name>
              <value>{{ master_ip }}</value>
            </property>
            <property>
              <name>yarn.nodemanager.aux-services</name>
              <value>mapreduce_shuffle</value>
            </property>
          </configuration>
    

    # Step 8: Start Hadoop DataNode and NodeManager services
    - name: Start DataNode service
      command: /usr/local/hadoop/sbin/hadoop-daemon.sh start datanode
      become_user: hadoop

    - name: Start NodeManager service
      command: /usr/local/hadoop/sbin/yarn-daemon.sh start nodemanager
      become_user: hadoop


