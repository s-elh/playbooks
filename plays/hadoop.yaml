---
- name: Installer et configurer Hadoop
  hosts: hadoop.localdomain
  become: yes
  become_method: sudo
  remote_user: "{{ ansible_user }}"  # Utilise l'utilisateur qui lance le playbook

  tasks:

    # Étape 1: Télécharger Hadoop
    - name: Télécharger Hadoop
      get_url:
        url: "https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6-aarch64.tar.gz"
        dest: "/opt/hadoop-3.3.6-aarch64.tar.gz"
        mode: '0644'

    # Étape 2: Extraire Hadoop
    - name: Extraire Hadoop
      unarchive:
        src: "/opt/hadoop-3.3.6-aarch64.tar.gz"
        dest: "/opt/"
        remote_src: yes
        mode: '0755'

    # Étape 3: Créer l'utilisateur Hadoop
    - name: Créer un utilisateur Hadoop
      user:
        name: hadoop
        group: hadoop
        shell: /bin/bash
        create_home: yes
        state: present
        append: yes

    # Étape 4: Créer le répertoire Hadoop
    - name: Créer le répertoire pour Hadoop
      file:
        path: "/opt/hadoop"
        state: directory
        owner: hadoop
        group: hadoop
        mode: '0755'

    # Étape 5: Créer le répertoire pour les fichiers de configuration Hadoop
    - name: Créer le répertoire pour les fichiers de configuration Hadoop
      file:
        path: "/opt/hadoop/etc/hadoop"
        state: directory
        owner: hadoop
        group: hadoop
        mode: '0755'

    # Étape 6: Télécharger et copier les fichiers de configuration Hadoop
    - name: Copier le fichier hadoop-env.sh
      copy:
        content: |
          #!/bin/bash
          export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
          export HADOOP_HOME=/opt/hadoop
          export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
          export HADOOP_MAPRED_HOME=$HADOOP_HOME
          export YARN_HOME=$HADOOP_HOME
          export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
        dest: "/opt/hadoop/etc/hadoop/hadoop-env.sh"
        owner: hadoop
        group: hadoop
        mode: '0644'

    - name: Copier le fichier core-site.xml
      copy:
        content: |
          <?xml version="1.0" encoding="UTF-8"?>
          <configuration>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://localhost:9000</value>
            </property>
          </configuration>
        dest: "/opt/hadoop/etc/hadoop/core-site.xml"
        owner: hadoop
        group: hadoop
        mode: '0644'

    - name: Copier le fichier hdfs-site.xml
      copy:
        content: |
          <?xml version="1.0" encoding="UTF-8"?>
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>1</value>
            </property>
            <property>
              <name>dfs.namenode.name.dir</name>
              <value>file:/opt/hadoop/hadoop_data/hdfs/namenode</value>
            </property>
            <property>
              <name>dfs.datanode.data.dir</name>
              <value>file:/opt/hadoop/hadoop_data/hdfs/datanode</value>
            </property>
          </configuration>
        dest: "/opt/hadoop/etc/hadoop/hdfs-site.xml"
        owner: hadoop
        group: hadoop
        mode: '0644'

    - name: Copier le fichier yarn-site.xml
      copy:
        content: |
          <?xml version="1.0" encoding="UTF-8"?>
          <configuration>
            <property>
              <name>yarn.resourcemanager.address</name>
              <value>localhost:8032</value>
            </property>
            <property>
              <name>yarn.nodemanager.aux-services</name>
              <value>mapreduce_shuffle</value>
            </property>
          </configuration>
        dest: "/opt/hadoop/etc/hadoop/yarn-site.xml"
        owner: hadoop
        group: hadoop
        mode: '0644'

    # Étape 7: Modifier les permissions sur Hadoop
    - name: Modifier les permissions sur Hadoop
      command: sudo chmod -R 755 /opt/hadoop
      become: yes
      become_method: sudo

    # Étape 8: Formater le Namenode HDFS
    - name: Formater le Namenode HDFS
      command: sudo /opt/hadoop/bin/hdfs namenode -format
      become: yes
      become_method: sudo

    # Étape 9: Démarrer le HDFS
    - name: Démarrer le HDFS
      command: sudo /opt/hadoop/sbin/start-dfs.sh
      become: yes
      become_method: sudo

    # Étape 10: Démarrer YARN
    - name: Démarrer YARN
      command: sudo /opt/hadoop/sbin/start-yarn.sh
      become: yes
      become_method: sudo

    # Étape 11: Vérifier le statut de Hadoop
    - name: Vérifier le statut de Hadoop
      command: sudo jps
      become: yes
      become_method: sudo
