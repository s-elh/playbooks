---
- name: Installer et configurer Hadoop
  hosts: all
  become: yes
  become_method: sudo
  remote_user: "{{ ansible_user }}"  # Utilise l'utilisateur qui lance le playbook

  vars:
    java_home: "/usr/lib/jvm/java-8-openjdk-amd64"
    hadoop_version: "3.3.6-aarch64"
    hadoop_dir: "/opt/hadoop-{{ hadoop_version }}"
    hadoop_tmp_dir: "/opt/hadoop-{{ hadoop_version }}/tmp"
    hadoop_namedir: "/opt/hadoop-{{ hadoop_version }}/data/namedir"
    hadoop_datanode_dir: "/opt/hadoop-{{ hadoop_version }}/data/datanode"
  
  tasks:
    # Vérification préalable de Java
    - name: Vérifier si Java est installé
      command: which java
      register: java_installed
      ignore_errors: yes
      changed_when: false

    - name: Installer OpenJDK 8 si Java n'est pas installé
      apt:
        name: openjdk-8-jdk
        state: present
        update_cache: yes
      when: java_installed.rc != 0
    
    # Étape 1: Télécharger Hadoop avec timeout augmenté
    - name: Télécharger Hadoop
      get_url:
        url: "https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6-aarch64.tar.gz"
        dest: "/opt/hadoop-{{ hadoop_version }}.tar.gz"
        mode: '0644'
        timeout: 600  # Augmenter le timeout à 10 minutes

    # Étape 2: Extraire Hadoop
    - name: Extraire Hadoop
      unarchive:
        src: "/opt/hadoop-{{ hadoop_version }}.tar.gz"
        dest: "/opt/"
        remote_src: yes
        mode: '0755'
        creates: "{{ hadoop_dir }}"  # Évite de réextraire si le répertoire existe déjà

    # Étape 3: Créer les répertoires nécessaires
    - name: Créer les répertoires nécessaires pour Hadoop
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
      loop:
        - "{{ hadoop_tmp_dir }}"
        - "{{ hadoop_namedir }}"
        - "{{ hadoop_datanode_dir }}"

    # Étape 4: Configurer hadoop-env.sh
    - name: Configurer hadoop-env.sh
      lineinfile:
        path: "{{ hadoop_dir }}/etc/hadoop/hadoop-env.sh"
        regexp: "^#?export JAVA_HOME="
        line: "export JAVA_HOME={{ java_home }}"
        create: yes  # Crée le fichier s'il n'existe pas

    # Étape 5: Configurer core-site.xml
    - name: Configurer core-site.xml
      copy:
        content: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
          <configuration>
              <property>
                  <name>fs.defaultFS</name>
                  <value>hdfs://{{ ansible_hostname }}:9000</value>
              </property>
              <property>
                  <name>hadoop.tmp.dir</name>
                  <value>{{ hadoop_tmp_dir }}</value>
              </property>
          </configuration>
        dest: "{{ hadoop_dir }}/etc/hadoop/core-site.xml"
        mode: '0644'

    # Étape 6: Configurer hdfs-site.xml
    - name: Configurer hdfs-site.xml
      copy:
        content: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
          <configuration>
              <property>
                  <name>dfs.replication</name>
                  <value>1</value>
              </property>
              <property>
                  <name>dfs.namenode.name.dir</name>
                  <value>{{ hadoop_namedir }}</value>
              </property>
              <property>
                  <name>dfs.datanode.data.dir</name>
                  <value>{{ hadoop_datanode_dir }}</value>
              </property>
              <property>
                  <name>dfs.webhdfs.enabled</name>
                  <value>true</value>
              </property>
          </configuration>
        dest: "{{ hadoop_dir }}/etc/hadoop/hdfs-site.xml"
        mode: '0644'

    # Étape 7: Configurer yarn-site.xml
    - name: Configurer yarn-site.xml
      copy:
        content: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
          <configuration>
              <property>
                  <name>yarn.resourcemanager.hostname</name>
                  <value>{{ ansible_hostname }}</value>
              </property>
              <property>
                  <name>yarn.resourcemanager.address</name>
                  <value>{{ ansible_hostname }}:8032</value>
              </property>
              <property>
                  <name>yarn.nodemanager.aux-services</name>
                  <value>mapreduce_shuffle</value>
              </property>
              <property>
                  <name>yarn.nodemanager.env-whitelist</name>
                  <value>HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_YARN_HOME</value>
              </property>
          </configuration>
        dest: "{{ hadoop_dir }}/etc/hadoop/yarn-site.xml"
        mode: '0644'

    # Étape 8: Configurer mapred-site.xml
    - name: Configurer mapred-site.xml
      copy:
        content: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
          <configuration>
              <property>
                  <name>mapreduce.framework.name</name>
                  <value>yarn</value>
              </property>
              <property>
                  <name>mapreduce.jobhistory.address</name>
                  <value>{{ ansible_hostname }}:10020</value>
              </property>
              <property>
                  <name>mapreduce.jobhistory.webapp.address</name>
                  <value>{{ ansible_hostname }}:19888</value>
              </property>
          </configuration>
        dest: "{{ hadoop_dir }}/etc/hadoop/mapred-site.xml"
        mode: '0644'

    # Étape 9: Formater le Namenode HDFS (seulement s'il n'a pas déjà été formaté)
    - name: Vérifier si le Namenode est déjà formaté
      stat:
        path: "{{ hadoop_namedir }}/current/VERSION"
      register: namenode_version

    - name: Formater le Namenode HDFS
      command: "{{ hadoop_dir }}/bin/hdfs namenode -format -force"
      when: not namenode_version.stat.exists
      register: format_result
      changed_when: format_result.rc == 0

    # Étape 10: Configurer SSH sans mot de passe (requis pour les scripts de démarrage)
    - name: Vérifier la présence de clés SSH
      stat:
        path: "~/.ssh/id_rsa"
      register: ssh_key_exists

    - name: Générer une paire de clés SSH si nécessaire
      command: ssh-keygen -t rsa -f ~/.ssh/id_rsa -N ""
      when: not ssh_key_exists.stat.exists

    - name: Ajouter la clé publique aux clés autorisées
      shell: cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
      args:
        creates: ~/.ssh/authorized_keys.added
      register: key_add
      
    - name: Marquer l'ajout de clé comme effectué
      file:
        path: ~/.ssh/authorized_keys.added
        state: touch
      when: key_add.changed

    # Étape 11: Démarrer le HDFS
    - name: Démarrer le HDFS
      command: "{{ hadoop_dir }}/sbin/start-dfs.sh"
      args:
        creates: /tmp/hadoop-dfs-started
      register: hdfs_start

    - name: Marquer le démarrage de HDFS
      file:
        path: /tmp/hadoop-dfs-started
        state: touch
      when: hdfs_start.changed

    # Étape 12: Démarrer YARN
    - name: Démarrer YARN
      command: "{{ hadoop_dir }}/sbin/start-yarn.sh"
      args:
        creates: /tmp/hadoop-yarn-started
      register: yarn_start

    - name: Marquer le démarrage de YARN
      file:
        path: /tmp/hadoop-yarn-started
        state: touch
      when: yarn_start.changed

    # Étape 13: Vérifier le statut de Hadoop
    - name: Vérifier le statut de Hadoop
      command: jps
      register: hadoop_status
      changed_when: false

    - name: Afficher le statut des processus Hadoop
      debug:
        var: hadoop_status.stdout_lines
