- name: Install and Configure Hadoop
  hosts: all
  become: yes
  vars:
    hadoop_version: "3.3.6"
    hadoop_url: "https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6-aarch64.tar.gz"
    hadoop_home: "/opt/hadoop"
    hadoop_group: "hadoop"

  tasks:
    # Installer les dépendances nécessaires
    - name: Install required dependencies
      apt:
        name:
          - openjdk-11-jdk
          - ssh
          - rsync
        state: present
        update_cache: yes

    # Télécharger Hadoop
    - name: Download Hadoop
      get_url:
        url: "{{ hadoop_url }}"
        dest: "/tmp/hadoop.tar.gz"

    # Extraire Hadoop
    - name: Extract Hadoop
      unarchive:
        src: "/tmp/hadoop.tar.gz"
        dest: "/opt/"
        remote_src: yes
        extra_opts: [--strip-components=1]

    # Définir les permissions sur le dossier Hadoop
    - name: Set ownership of Hadoop directory
      file:
        path: "{{ hadoop_home }}"
        owner: "{{ ansible_user }}"
        group: "{{ hadoop_group }}"
        state: directory
        recurse: yes

    # Créer le répertoire de configuration
    - name: Create configuration directory
      file:
        path: "/etc/hadoop"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ hadoop_group }}"

    # Copier les fichiers de configuration Hadoop
    - name: Deploy core-site.xml
      copy:
        dest: "/etc/hadoop/core-site.xml"
        content: |
          <configuration>
              <property>
                  <name>fs.defaultFS</name>
                  <value>hdfs://localhost:9000</value>
              </property>
          </configuration>

    - name: Deploy hdfs-site.xml
      copy:
        dest: "/etc/hadoop/hdfs-site.xml"
        content: |
          <configuration>
              <property>
                  <name>dfs.replication</name>
                  <value>1</value>
              </property>
              <property>
                  <name>dfs.namenode.name.dir</name>
                  <value>/opt/hadoop/dfs/name</value>
              </property>
              <property>
                  <name>dfs.datanode.data.dir</name>
                  <value>/opt/hadoop/dfs/data</value>
              </property>
          </configuration>

    - name: Deploy mapred-site.xml
      copy:
        dest: "/etc/hadoop/mapred-site.xml"
        content: |
          <configuration>
              <property>
                  <name>mapreduce.framework.name</name>
                  <value>yarn</value>
              </property>
          </configuration>

    - name: Deploy yarn-site.xml
      copy:
        dest: "/etc/hadoop/yarn-site.xml"
        content: |
          <configuration>
              <property>
                  <name>yarn.nodemanager.aux-services</name>
                  <value>mapreduce_shuffle</value>
              </property>
          </configuration>

    - name: Deploy hadoop-env.sh
      copy:
        dest: "/etc/hadoop/hadoop-env.sh"
        content: |
          export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
          export HADOOP_HOME={{ hadoop_home }}
          export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
        mode: '0755'

    # Ajouter les variables d'environnement Hadoop
    - name: Configure environment variables for Hadoop
      lineinfile:
        path: /etc/profile.d/hadoop.sh
        line: 'export HADOOP_HOME={{ hadoop_home }}'
        create: yes

    # Appliquer les variables d'environnement
    - name: Load environment variables
      shell: "source /etc/profile.d/hadoop.sh"
      args:
        executable: /bin/bash

    # Formater le namenode
    - name: Format the HDFS namenode
      command: "{{ hadoop_home }}/bin/hdfs namenode -format"
      become_user: "{{ ansible_user }}"

    # Démarrer les services Hadoop
    - name: Start Hadoop services
      command: "{{ hadoop_home }}/sbin/start-dfs.sh"
      become_user: "{{ ansible_user }}"

    - name: Start YARN
      command: "{{ hadoop_home }}/sbin/start-yarn.sh"
      become_user: "{{ ansible_user }}"
