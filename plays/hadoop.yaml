---
- name: Prepare Hadoop Installation
  hosts: all
  become: yes
  vars:
    hadoop_user: "hadoop"
    hadoop_group: "hadoopgroup"
    hadoop_home: "/home/hadoop"
    hadoop_version: "3.3.6"
    java_version: "11"
    hadoop_mirrors:
      - "https://dlcdn.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
      - "https://downloads.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
      - "https://archive.apache.org/dist/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"

  pre_tasks:
    - name: Update package cache
      apt:
        update_cache: yes

    - name: Install required system packages
      apt:
        name:
          - wget
          - software-properties-common
        state: present

    - name: Install Java
      apt:
        name:
          - openjdk-{{ java_version }}-jdk
          - openjdk-{{ java_version }}-jre
        state: present

    - name: Set Java environment variables
      blockinfile:
        path: /etc/environment
        block: |
          JAVA_HOME=/usr/lib/jvm/java-{{ java_version }}-openjdk-amd64
          PATH="$PATH:$JAVA_HOME/bin"
        create: yes

    - name: Ensure group exists
      group:
        name: "{{ hadoop_group }}"
        state: present

    - name: Create Hadoop user with password
      user:
        name: "{{ hadoop_user }}"
        password: "{{ 'hadoop' | password_hash('sha512') }}"
        group: "{{ hadoop_group }}"
        home: "{{ hadoop_home }}"
        shell: /bin/bash
        create_home: yes
        state: present

  tasks:
    - name: Generate SSH key for Hadoop user
      command: ssh-keygen -t rsa -b 2048 -f {{ hadoop_home }}/.ssh/id_rsa -N ""
      args:
        creates: "{{ hadoop_home }}/.ssh/id_rsa"
      become_user: "{{ hadoop_user }}"

    - name: Manually add SSH key for Hadoop user
      command: "cat {{ hadoop_home }}/.ssh/id_rsa.pub >> {{ hadoop_home }}/.ssh/authorized_keys"
      become_user: "{{ hadoop_user }}"

    - name: Fix permissions on .ssh directory
      file:
        path: "{{ hadoop_home }}/.ssh"
        state: directory
        mode: '0700'
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"

    - name: Fix permissions on authorized_keys
      file:
        path: "{{ hadoop_home }}/.ssh/authorized_keys"
        state: file
        mode: '0600'
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"

    - name: Download Hadoop using wget with multiple mirrors
      shell: |
        set -o pipefail
        for mirror in {{ hadoop_mirrors | join(' ') }}; do
          wget --tries=3 \
               --timeout=300 \
               --no-check-certificate \
               -O "{{ hadoop_home }}/hadoop-{{ hadoop_version }}.tar.gz" \
               "$mirror" && exit 0
        done
        exit 1
      args:
        creates: "{{ hadoop_home }}/hadoop-{{ hadoop_version }}.tar.gz"
        executable: /bin/bash
      register: wget_download
      changed_when: wget_download.rc == 0
      failed_when: wget_download.rc != 0

    - name: Extract Hadoop archive
      unarchive:
        src: "{{ hadoop_home }}/hadoop-{{ hadoop_version }}.tar.gz"
        dest: "/usr/local/"
        remote_src: yes
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"

    - name: Set Hadoop environment variables
      blockinfile:
        path: "{{ hadoop_home }}/.bashrc"
        block: |
          export HADOOP_HOME="/usr/local/hadoop"
          export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
        create: yes
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"

    - name: Configure Hadoop (core-site.xml)
      copy:
        dest: "/usr/local/hadoop/etc/hadoop/core-site.xml"
        content: |
          <configuration>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://localhost:9000</value>
            </property>
          </configuration>
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"

    - name: Configure Hadoop (hdfs-site.xml)
      copy:
        dest: "/usr/local/hadoop/etc/hadoop/hdfs-site.xml"
        content: |
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>1</value>
            </property>
            <property>
              <name>dfs.namenode.name.dir</name>
              <value>file:///usr/local/hadoop/data/namenode</value>
            </property>
            <property>
              <name>dfs.datanode.data.dir</name>
              <value>file:///usr/local/hadoop/data/datanode</value>
            </property>
          </configuration>
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"

    - name: Format Hadoop Namenode
      command: "/usr/local/hadoop/bin/hdfs namenode -format"
      become_user: "{{ hadoop_user }}"
      args:
        creates: "/usr/local/hadoop/data"

    - name: Start Hadoop Services
      shell: |
        /usr/local/hadoop/sbin/start-dfs.sh
        /usr/local/hadoop/sbin/start-yarn.sh
      become_user: "{{ hadoop_user }}"

  handlers:
    - name: Restart SSH
      service:
        name: ssh
        state: restarted
