---
- name: Complete Hadoop Installation and Configuration for Ubuntu
  hosts: all
  become: yes
  vars:
    hadoop_user: "hdoop"
    hadoop_group: "hadoopgroup"
    hadoop_home: "/home/hdoop"
    hadoop_version: "3.3.6"
    java_home: "/usr/lib/jvm/java-11-openjdk-amd64"
    hadoop_install_dir: "/home/hdoop/hadoop"
    hadoop_mirrors:
      - "https://dlcdn.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
      - "https://downloads.apache.org/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
      - "https://archive.apache.org/dist/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"

  pre_tasks:
    - name: Update Ubuntu apt cache
      apt:
        update_cache: yes

    - name: Install Java and required packages
      apt:
        name: 
          - default-jdk
          - default-jre
          - wget
          - openssh-server
          - ssh
          - rsync
        state: present

    - name: Ensure group exists
      group:
        name: "{{ hadoop_group }}"
        state: present

    - name: Create Hadoop user
      user:
        name: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"
        home: "{{ hadoop_home }}"
        shell: /bin/bash
        create_home: yes
        state: present

  tasks:
    - name: Create Hadoop download directory
      file:
        path: "{{ hadoop_home }}/downloads"
        state: directory
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"
        mode: '0755'

    - name: Download Hadoop using wget with multiple mirrors
      shell: |
        set -o pipefail
        for mirror in {{ hadoop_mirrors | join(' ') }}; do
          wget --tries=3 \
               --timeout=300 \
               --no-check-certificate \
               -O "{{ hadoop_home }}/downloads/hadoop-{{ hadoop_version }}.tar.gz" \
               "$mirror" && exit 0
        done
        exit 1
      args:
        creates: "{{ hadoop_home }}/downloads/hadoop-{{ hadoop_version }}.tar.gz"
        executable: /bin/bash
      register: wget_download
      changed_when: wget_download.rc == 0
      failed_when: wget_download.rc != 0

    - name: Extract Hadoop archive
      unarchive:
        src: "{{ hadoop_home }}/downloads/hadoop-{{ hadoop_version }}.tar.gz"
        dest: "{{ hadoop_home }}"
        remote_src: yes
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"

    - name: Create symbolic link for Hadoop
      file:
        src: "{{ hadoop_home }}/hadoop-{{ hadoop_version }}"
        dest: "{{ hadoop_install_dir }}"
        state: link
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"

    - name: Create necessary directories for HDFS
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"
        mode: '0755'
      loop:
        - "{{ hadoop_home }}/hdfs/namenode"
        - "{{ hadoop_home }}/hdfs/datanode"
        - "{{ hadoop_home }}/hdfs/tmp"

    - name: Configure .bashrc for Hadoop environment
      blockinfile:
        path: "{{ hadoop_home }}/.bashrc"
        block: |
          # Hadoop Environment Variables
          export JAVA_HOME={{ java_home }}
          export HADOOP_HOME={{ hadoop_install_dir }}
          export HADOOP_INSTALL=$HADOOP_HOME
          export HADOOP_MAPRED_HOME=$HADOOP_HOME
          export HADOOP_COMMON_HOME=$HADOOP_HOME
          export HADOOP_HDFS_HOME=$HADOOP_HOME
          export YARN_HOME=$HADOOP_HOME
          export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
          export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
          export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
        marker: "# {mark} ANSIBLE MANAGED HADOOP BLOCK"
        create: yes
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"

    - name: Configure hadoop-env.sh
      copy:
        dest: "{{ hadoop_install_dir }}/etc/hadoop/hadoop-env.sh"
        content: |
          # Set Hadoop-specific environment variables here

          # The only required environment variable is JAVA_HOME.  All others are
          # optional.  When running a distributed configuration it is best to
          # set JAVA_HOME in this file, so that it is correctly defined on
          # remote nodes.

          # The java implementation to use.
          export JAVA_HOME={{ java_home }}

          # Extra Java CLASSPATH elements.  Automatically insert capacity-scheduler.
          export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/local/hadoop/lib/*.jar

          # Extra Java runtime options.  Empty by default.
          export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"
        mode: '0644'

    - name: Configure core-site.xml
      copy:
        dest: "{{ hadoop_install_dir }}/etc/hadoop/core-site.xml"
        content: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
          <configuration>
              <property>
                  <name>fs.defaultFS</name>
                  <value>hdfs://localhost:9000</value>
              </property>
              <property>
                  <name>hadoop.tmp.dir</name>
                  <value>{{ hadoop_home }}/hdfs/tmp</value>
              </property>
          </configuration>
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"
        mode: '0644'

    - name: Configure hdfs-site.xml
      copy:
        dest: "{{ hadoop_install_dir }}/etc/hadoop/hdfs-site.xml"
        content: |
          <?xml version="1.0" encoding="UTF-8"?>
          <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
          <configuration>
              <property>
                  <name>dfs.replication</name>
                  <value>1</value>
              </property>
              <property>
                  <name>dfs.namenode.name.dir</name>
                  <value>{{ hadoop_home }}/hdfs/namenode</value>
              </property>
              <property>
                  <name>dfs.datanode.data.dir</name>
                  <value>{{ hadoop_home }}/hdfs/datanode</value>
              </property>
          </configuration>
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"
        mode: '0644'

    - name: Set ownership of Hadoop installation
      file:
        path: "{{ hadoop_install_dir }}"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_group }}"
        recurse: yes

  post_tasks:
    - name: Verify Hadoop installation
      stat:
        path: "{{ hadoop_install_dir }}"
      register: hadoop_dir

    - name: Fail if Hadoop directory does not exist
      fail:
        msg: "Hadoop installation failed - directory not found"
      when: not hadoop_dir.stat.exists
